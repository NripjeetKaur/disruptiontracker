{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f9a740-7e97-4f7a-a021-a8d61c40ff6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GoogleNews in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (1.6.15)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from GoogleNews) (4.12.3)\n",
      "Requirement already satisfied: dateparser in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from GoogleNews) (1.2.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from GoogleNews) (2.9.0.post0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from beautifulsoup4->GoogleNews) (2.5)\n",
      "Requirement already satisfied: pytz>=2024.2 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from dateparser->GoogleNews) (2025.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from dateparser->GoogleNews) (2024.9.11)\n",
      "Requirement already satisfied: tzlocal>=0.2 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from dateparser->GoogleNews) (5.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from python-dateutil->GoogleNews) (1.16.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from tzlocal>=0.2->dateparser->GoogleNews) (2023.3)\n",
      "Found 10 articles\n",
      "                                            headline\n",
      "0  Walmart and the Trump Administration: A New Er...\n",
      "1  Let them eat junk food: Major organic supplier...\n",
      "2  Walmart tells Chinese suppliers to resume ship...\n",
      "3   Amazon and Walmart Battle for the Hybrid Shopper\n",
      "4  Walmart, Target And Home Depot CEOs Meet With ...\n"
     ]
    }
   ],
   "source": [
    "!pip install GoogleNews\n",
    "\n",
    "from GoogleNews import GoogleNews\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Google News\n",
    "googlenews = GoogleNews(lang='en', period='7d')  # News from last 7 days\n",
    "googlenews.search('Walmart supply chain disruption')\n",
    "\n",
    "results = googlenews.results()\n",
    "print(f\"Found {len(results)} articles\")\n",
    "\n",
    "# Extract Headlines\n",
    "label=[]\n",
    "headlines = []\n",
    "for article in results:\n",
    "    headlines.append(article['title'])\n",
    "\n",
    "# Store in DataFrame\n",
    "df = pd.DataFrame({'headline': headlines})\n",
    "df.to_csv('index.csv', index=False)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bcc94d7-08f9-4ab7-ba11-fe3dad9b78c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GoogleNews in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (1.6.15)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from GoogleNews) (4.12.3)\n",
      "Requirement already satisfied: dateparser in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from GoogleNews) (1.2.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from GoogleNews) (2.9.0.post0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from beautifulsoup4->GoogleNews) (2.5)\n",
      "Requirement already satisfied: pytz>=2024.2 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from dateparser->GoogleNews) (2025.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from dateparser->GoogleNews) (2024.9.11)\n",
      "Requirement already satisfied: tzlocal>=0.2 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from dateparser->GoogleNews) (5.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from python-dateutil->GoogleNews) (1.16.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from tzlocal>=0.2->dateparser->GoogleNews) (2023.3)\n",
      "Searching for: Port closure\n",
      "Searching for: Shipping delays\n",
      "Searching for: Geopolitical unrest supply chain\n",
      "Searching for: Strikes at logistics hubs\n",
      "    search_term                                           headline\n",
      "0  Port closure  Kids punching a port-a-potty and complaints ab...\n",
      "1  Port closure  New Veracruz screwworm case cause border closu...\n",
      "2  Port closure  CHS Superior Terminal to close according to Po...\n",
      "3  Port closure  Port Authority: Closure Of CHS Superior Termin...\n",
      "4  Port closure  Southern Border Livestock Ports Closed After N...\n",
      "Total headlines collected: 40\n"
     ]
    }
   ],
   "source": [
    "!pip install GoogleNews\n",
    "from GoogleNews import GoogleNews\n",
    "import pandas as pd\n",
    "import time  # To avoid rate limiting\n",
    "\n",
    "search_terms = [\n",
    "    \"Port closure\",\n",
    "    \"Shipping delays\",\n",
    "    \"Geopolitical unrest supply chain\",\n",
    "    \"Strikes at logistics hubs\"\n",
    "]\n",
    "\n",
    "all_headlines = []\n",
    "\n",
    "googlenews = GoogleNews(lang='en', period='7d')  # News from last 7 days\n",
    "\n",
    "for term in search_terms:\n",
    "    print(f\"Searching for: {term}\")\n",
    "    googlenews.clear()  # Clear previous search results\n",
    "    googlenews.search(term)\n",
    "    results = googlenews.results()\n",
    "    \n",
    "    for article in results:\n",
    "        headline = article['title']\n",
    "        link = article['link']\n",
    "        all_headlines.append({\"search_term\": term, \"headline\": headline })\n",
    "    \n",
    "    time.sleep(2)  # Pause to avoid overwhelming servers\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_headlines)\n",
    "print(df.head())\n",
    "df['label'] = ''  # Adds a column with empty strings as placeholder labels\n",
    "df.to_csv('index.csv', index=False)\n",
    "print(f\"Total headlines collected: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dac7f1e-e20c-42c5-a4ce-b875e259ab96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map: {}\n"
     ]
    }
   ],
   "source": [
    "'''df = df.dropna(subset=['label'])               # Remove rows with missing labels\n",
    "df['label'] = df['label'].astype(str)          # Make sure labels are strings\n",
    "df['label_id'] = encoder.fit_transform(df['label'])'''\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Drop missing labels\n",
    "df = df.dropna(subset=['label'])\n",
    "\n",
    "# 2. Strip whitespace and force all labels to strings\n",
    "df['label'] = df['label'].astype(str).str.strip()\n",
    "\n",
    "# 3. Filter out any empty labels that may have snuck in\n",
    "df = df[df['label'] != '']\n",
    "\n",
    "# 4. Re-encode clean labels\n",
    "encoder = LabelEncoder()\n",
    "df['label_id'] = encoder.fit_transform(df['label'])\n",
    "\n",
    "# (Optional) View mapping\n",
    "label_map = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(\"Label map:\", label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "161146f1-ddbd-4e3b-90ac-cee9c4b8951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    search_term                                           headline  label\n",
      "0  Port closure  Kids punching a port-a-potty and complaints ab...    NaN\n",
      "1  Port closure  New Veracruz screwworm case cause border closu...    NaN\n",
      "2  Port closure  CHS Superior Terminal to close according to Po...    NaN\n",
      "3  Port closure  Port Authority: Closure Of CHS Superior Termin...    NaN\n",
      "4  Port closure  Southern Border Livestock Ports Closed After N...    NaN\n",
      "Index(['search_term', 'headline', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load your uploaded file\n",
    "df = pd.read_csv('index.csv')\n",
    "\n",
    "# Preview\n",
    "print(df.head())\n",
    "print(df.columns)  # Check if the label column exists\n",
    "# Drop missing labels\n",
    "df = df.dropna(subset=['label'])\n",
    "\n",
    "# Ensure labels are integers\n",
    "df['label'] = df['label'].astype(str)\n",
    "df['headline'] = df['headline'].astype(str)\n",
    "texts = list(df['headline'])\n",
    "\n",
    "\n",
    "df.loc[0, 'label'] = 'Port closure'\n",
    "df.loc[1, 'label'] = 'Shipping delays'\n",
    "df.loc[3, 'label'] = 'Geopolitical unrest supply chain'\n",
    "df.loc[2, 'label'] = 'Strikes at logistics hubs'\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['label_id'] = encoder.fit_transform(df['label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61fdec8c-8382-4641-97a7-0f3445d6c598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([4, 3])\n",
      "Labels shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Convert labels to tensor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['label_id'] = encoder.fit_transform(df['label'])\n",
    "labels = torch.tensor(df['label_id'].tolist())\n",
    "#labels = torch.tensor(df['label'].tolist())\n",
    "\n",
    "# Show tensor shapes\n",
    "print(\"Input IDs shape:\", encodings['input_ids'].shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['label_id'] = encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2858a3aa-8600-4c61-b3fe-d103b84ce2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Geopolitical unrest supply chain': 0, 'Port closure': 1, 'Shipping delays': 2, 'Strikes at logistics hubs': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['label_id'] = encoder.fit_transform(df['label'])\n",
    "label_map = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(label_map)  # This will show your label-to-ID mapping\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['headline'],\n",
    "    df['label_id'],  # <-- Use label_id here\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6b80862-f7c0-4d46-879f-8499e3fd15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "\n",
    "# Force all headlines to strings\n",
    "df['headline'] = df['headline'].astype(str)\n",
    "\n",
    "# Drop any missing or empty values just in case\n",
    "df = df.dropna(subset=['headline'])\n",
    "df = df[df['headline'].str.strip() != '']\n",
    "\n",
    "# Convert to a list of strings\n",
    "texts = list(df['headline'])  # Guaranteed to be list[str]\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "encodings = tokenizer(\n",
    "    list(df['headline']),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5386807-701f-462c-be75-89030c9a3519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.int64\n",
      "token_type_ids: torch.int64\n",
      "attention_mask: torch.int64\n",
      "labels: torch.int64\n",
      "input_ids: torch.int64\n",
      "token_type_ids: torch.int64\n",
      "attention_mask: torch.int64\n",
      "labels: torch.int64\n",
      "input_ids: torch.int64\n",
      "token_type_ids: torch.int64\n",
      "attention_mask: torch.int64\n",
      "labels: torch.int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manya verma\\AppData\\Local\\Temp\\ipykernel_21580\\3947210762.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HeadlineDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        for k, v in item.items():\n",
    "            print(f\"{k}: {v.dtype}\")  # Should all be Long\n",
    "        return item\n",
    "  \n",
    "dataset = HeadlineDataset(encodings, df['label_id'].tolist())\n",
    "remove_unused_columns=False\n",
    "\n",
    "# Trigger some samples to invoke __getitem__\n",
    "for i in range(3):\n",
    "    _ = dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eeb120f1-fd08-4013-9fcb-5739064052a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['label_id'] = encoder.fit_transform(df['label'])\n",
    "\n",
    "train_texts, val_texts, train_labels_raw, val_labels_raw = train_test_split(\n",
    "    df['headline'], df['label_id'], test_size=0.2, random_state=42\n",
    ")\n",
    "# Tokenize using previously split text\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, return_tensors='pt')\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# Use these correct labels\n",
    "train_labels = torch.tensor(train_labels_raw.tolist(), dtype=torch.long)\n",
    "val_labels = torch.tensor(val_labels_raw.tolist(), dtype=torch.long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86095d32-273f-4308-9bc2-0e700f774c89",
   "metadata": {},
   "source": [
    "'''# Tokenize\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, return_tensors='pt')\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "train_labels = torch.tensor(train_labels.tolist())\n",
    "val_labels = torch.tensor(val_labels.tolist())'''\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e99ef45-3cab-4a62-919a-4b398bc0aa4d",
   "metadata": {},
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aba254ee-5542-4169-8eb7-d2340f546a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_labels[0]))  # Should be int, not float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60bdd5ca-4433-453a-a654-4257c92a1f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (4.53.0)\n",
      "Requirement already satisfied: torch in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manya verma\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Label mapping: {nan: 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Manya verma\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.013800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manya verma\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./bert_model\\\\tokenizer_config.json',\n",
       " './bert_model\\\\special_tokens_map.json',\n",
       " './bert_model\\\\vocab.txt',\n",
       " './bert_model\\\\added_tokens.json',\n",
       " './bert_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers torch scikit-learn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['label_id'] = df['label_id'].astype(int)\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv('index.csv')\n",
    "\n",
    "# Encode string labels as integers\n",
    "encoder = LabelEncoder()\n",
    "df['label_id'] = encoder.fit_transform(df['label'])\n",
    "\n",
    "# Save the label-to-ID mapping\n",
    "label_map = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(\"Label mapping:\", label_map)\n",
    "# Split data\n",
    "texts = list(df['headline'])\n",
    "labels = list(df['label_id'])\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize headlines\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "import torch\n",
    "\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # ✅ This should be Long\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_encodings, train_labels)\n",
    "val_dataset = NewsDataset(val_encodings, val_labels)\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3) # num_labels=len(label_map)\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "\n",
    "trainer.evaluate()\n",
    "\n",
    "model.save_pretrained('./bert_model')\n",
    "tokenizer.save_pretrained('./bert_model')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0098aa11-c42b-48b8-81af-7f41d6c53332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnp_int = np.int64(42)\\nnp_float = np.float64(3.14)\\npy_int=int(np_int)\\npy_float=float(np_float)\\n# Save to JSON\\nwith open(\"label_map.json\", \"w\") as f:\\n    json.dump(label_map, f)\\n\\n#my_numpy_int = np.int64(42)\\nmy_dict = {int(my_numpy_int): \"Label A\"}  # ✅ Works fine\\nimport json\\nlabel_map = dict(zip(encoder.transform(encoder.classes_), encoder.classes_))  # ID to label\\nwith open(\"label_map.json\", \"w\") as f:\\n    json.dump(label_map, f)\\n    '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    " # Convert NumPy int64 keys to Python int\n",
    "label_map = {\n",
    "    int(k): v\n",
    "    for k, v in zip(encoder.transform(encoder.classes_), encoder.classes_)\n",
    "}\n",
    "'''\n",
    "np_int = np.int64(42)\n",
    "np_float = np.float64(3.14)\n",
    "py_int=int(np_int)\n",
    "py_float=float(np_float)\n",
    "# Save to JSON\n",
    "with open(\"label_map.json\", \"w\") as f:\n",
    "    json.dump(label_map, f)\n",
    "\n",
    "#my_numpy_int = np.int64(42)\n",
    "my_dict = {int(my_numpy_int): \"Label A\"}  # ✅ Works fine\n",
    "import json\n",
    "label_map = dict(zip(encoder.transform(encoder.classes_), encoder.classes_))  # ID to label\n",
    "with open(\"label_map.json\", \"w\") as f:\n",
    "    json.dump(label_map, f)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cb9e9673-9c9a-4707-b00b-0587f7b4eefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.53.0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: C:\\Users\\Manya verma\\anaconda3\\Lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n",
      "---\n",
      "Name: accelerate\n",
      "Version: 1.8.1\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: zach.mueller@huggingface.co\n",
      "License: Apache\n",
      "Location: C:\\Users\\Manya verma\\anaconda3\\Lib\\site-packages\n",
      "Requires: huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by: \n",
      "---\n",
      "Name: torch\n",
      "Version: 2.7.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: C:\\Users\\Manya verma\\anaconda3\\Lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, setuptools, sympy, typing-extensions\n",
      "Required-by: accelerate\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1491e67c-f70b-43b7-8f4c-eb55e6b88392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.int64\n",
      "token_type_ids: torch.int64\n",
      "attention_mask: torch.int64\n",
      "labels: torch.int64\n",
      "tensor([[0.1501]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(train_dataset, batch_size=1)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "for k, v in batch.items():\n",
    "    print(f\"{k}: {v.dtype}\")  # Should all be torch.int64\n",
    "\n",
    "output = model(**batch)\n",
    "print(output.logits)  # If this runs, Trainer should work too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b5724581-40ed-49c0-ab34-33a4745241b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manya verma\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.206700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=0.18203331157565117, metrics={'train_runtime': 35.9662, 'train_samples_per_second': 2.669, 'train_steps_per_second': 0.334, 'total_flos': 1282677911424.0, 'train_loss': 0.18203331157565117, 'epoch': 3.0})"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1337405d-8693-48ac-9aca-b0a9bd793105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97eb564b-a9b0-4df0-8fcd-a6f52d47154f",
   "metadata": {},
   "source": [
    "outputs = trainer.predict(val_dataset)\n",
    "logits = outputs.predictions  # Raw scores from the model\n",
    "import torch\n",
    "\n",
    "predicted_ids = torch.argmax(torch.tensor(logits), dim=1)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "predicted_labels = encoder.inverse_transform(predicted_ids.tolist())\n",
    "true_labels = encoder.inverse_transform(val_labels)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation F1 Score: {f1:.4f}\")\n",
    "for i in range(5):\n",
    "    print(f\"Headline: {val_texts[i]}\")\n",
    "    print(f\"Predicted Label: {predicted_labels[i]}\")\n",
    "    print(f\"True Label: {true_labels[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2cbe7a-f360-4d2c-bb5d-f7dfc047f80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd56b6d-72cb-470d-9ab7-3e9d5eeb53ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
